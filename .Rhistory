# Evaluating model
model_test <- predict(model_train, newdata = test_set)
confusion_matrix <- table(model_test, test_set[, target])
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Update the best model if necessary
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model_train
best_ntree <- ntree
best_mtry <- mtry
}
} # End of for loop
}, error = function(e){
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during Random Optimization training:", e$message),
type = "error", showCancelButton = FALSE)
progress$close()
return(NULL)
}) # End of tryCatch
progress$set(message = "Generating Predictions", value = 0.7)
model_test <- predict(best_model, newdata = test_set)
confusion_matrix <- confusionMatrix(data = model_test,
as.factor(test_set[, target]))
return_list <- list(model_train = best_model, model_test = model_test,
confusion_matrix = confusion_matrix, training_data = training_set,
test_data = test_set, full_data = df, loan_default = target,
formula = formula, accuracy = best_accuracy,
ntree = best_ntree, mtry = best_mtry)
} # End of if statement
else{
tryCatch({
model_train <- randomForest(formula = formula, data = training_set)
}, error = function(e){
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during Random Forest training:", e$message),
type = "error", showCancelButton = FALSE)
progress$close()
return(NULL)
}) # End of tryCatch
progress$set(message = "Generating Predictions", value = 0.7)
model_test <- predict(model_train, newdata = test_set)
confusion_matrix <- confusionMatrix(data = model_test,
as.factor(test_set[, target]))
return_list <- list(model_train = model_train, model_test = model_test,
confusion_matrix = confusion_matrix, training_data = training_set,
test_data = test_set, full_data = df, loan_default = target,
formula = formula)
}
progress$set(message = "Outputting Model", value = 0.9)
Sys.sleep(0.75)
progress$close()
} # End of Random Forest
else if(input$modelSelection == "XGBoost"){
progress$set(message = "Running XGBoost", value = 0.4)
Sys.sleep(0.75)
tryCatch({
# Small data processing to create correct y & yvals reference levels
training_set <- training_set %>%
mutate(!!sym(target) := recode(!!sym(target), "Default" = 0, "Non_Default" = 1),
!!sym(target) := as.numeric(as.factor(!!sym(target))),
!!sym(target) := !!sym(target) - 1)
test_set <- test_set %>%
mutate(!!sym(target) := recode(!!sym(target), "Default" = 0, "Non_Default" = 1),
!!sym(target) := as.numeric(as.factor(!!sym(target))),
!!sym(target) := !!sym(target) - 1)
}, error = function(e){
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during XGBoost data processing:", e$message),
type = "error", showCancelButton = FALSE)
progress$close()
return(NULL)
})
# Creating model matrices
x <- model.matrix(formula, data = training_set)
y <- model.frame(formula, data = training_set)[, target]
xvals <- model.matrix(formula, data = test_set)
yvals <- model.frame(formula, data = test_set)[, target]
progress$set(message = "Training Model", value = 0.5)
Sys.sleep(0.75)
# Optimizing using Bayesian Optimization
if(input$bayes == TRUE){
set.seed(112)
progress$set(message = "Applying Bayesian Optimisation", value = 0.6)
Sys.sleep(0.75)
tryCatch({
# Function will take the tuning parameters as an input and return the best
# cross validation results
scoring_function <- function(
eta, gamma, max_depth, min_child_weight, subsample, nfold) {
dtrain <- xgb.DMatrix(x, label = y, missing = NA)
pars <- list(
eta = eta,
gamma = gamma,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample,
booster = "gbtree",
objective = "binary:logistic",
eval_metric = "auc",
verbosity = 0
)
xgbcv <- xgb.cv(
params = pars,
data = dtrain,
nfold = nfold,
nrounds = 100,
prediction = TRUE,
showsd = TRUE,
early_stopping_rounds = 10,
maximize = TRUE,
stratified = TRUE
)
# required by the package, the output must be a list
# with at least one element of "Score", the measure to optimize
# Score must start with capital S
# For this case, we also report the best number of iterations
return(list(Score = max(xgbcv$evaluation_log$test_auc_mean),
nrounds = xgbcv$best_iteration))
} # End of scoring_function()
# Setting tuning boundaries
bounds <- list(
eta = c(0, 1),
gamma = c(0, 100),
max_depth = c(2L, 10L), # L means integers
min_child_weight = c(1, 25),
subsample = c(0.25, 1),
nfold = c(3L, 10L)
)
# Running the optimization with a time counter
set.seed(201)
overall_time <- system.time(
output <- bayesOpt(
FUN = scoring_function,
bounds = bounds,
initPoints = 7, # Must be higher than out function inputs
iters.n = input$bayesIter, # Can be set to any iteration value
))
# Outputting best parameters
best_parameters <- getBestPars(output)
# Fitting a tuned model --------------------------------------------------------
params <- list(eta = best_parameters[1],
gamma = best_parameters[2],
max_depth = best_parameters[3],
min_child_weight = best_parameters[4],
subsample = best_parameters[5],
nfold = best_parameters[6],
objective = "binary:logistic")
numrounds <- output$scoreSummary$nrounds[
which(output$scoreSummary$Score == max(output$scoreSummary$Score))]
model_train <- tryCatch({
xgboost::xgboost(data = x,
label = y,
params = params,
nrounds = numrounds,
eval_metric = "auc")
}, error = function(e) {
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during XGBoost training:", e$message),
type = "error", showCancelButton = FALSE)
progress$close()
return(NULL)
}) # End of tryCatch {model_train}
xgbcv <- NULL
}, error = function(e) {
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during Bayesian Optimization:", e$message),
type = "error", showCancelButton = FALSE)
progress$close()
return(NULL)
}) # End of tryCatch (Bayesian Optimization)
} # end of if statement
else{
set.seed(112)
# Set the XGBoost parameters
params <- list(
max_depth = 6,                               # Default
eta = 0.3,                                   # Default
gamma = 0,                                   # Default
min_child_weight = 1,                        # Default
subsample = 1,                               # Default
booster = "gbtree",                          # Default
objective = "binary:logistic",               # Binary classification
eval_metric = "auc",                         # Metric to evaluate model performance
verbosity = 0                                # Verbosity of printing messages
)
# Using cross validation to find best number of rounds
# with potential error handling
tryCatch({
xgbcv <- xgb.cv(data = x,
label = y,
params = params,
nrounds = 100, prediction = TRUE, showsd = TRUE,
early_stopping_rounds = 10,
maximize = TRUE, nfold = 10, stratified = TRUE)
}, error = function(e){
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during cross-validation:", e$message),
type = "error", showCancelButton = FALSE)
return(NULL)
}) # End of tryCatch
# Extracting optimal number of rounds
numrounds <- min(which(
xgbcv$evaluation_log$test_auc_mean == max(xgbcv$evaluation_log$test_auc_mean)))
tryCatch({
# Running model with default parameters
model_train <- xgboost::xgboost(data = x,
label = y,
params = params,
nrounds = numrounds)
}, error = function(e){
shinyalert::shinyalert(title = "Oops", text = paste("Error occurred during XGBoost model training:", e$message),
type = "error", showCancelButton = FALSE)
return(NULL)
}) # End of tryCatch
} # end of else statement
# Evaluating model
progress$set(message = "Generating Predictions", value = 0.7)
Sys.sleep(0.75)
model_test <- predict(model_train, xvals, type = "response")
confusion_matrix <- confusionMatrix(as.factor(round(model_test)), as.factor(yvals))
progress$set(message = "Outputting Information", value = 0.9)
Sys.sleep(0.75)
return_list <- list(model_train = model_train, model_test = model_test,
confusion_matrix = confusion_matrix, training_data = training_set,
test_data = test_set, full_data = df, loan_default = target,
formula = formula, x = x, y = y, xvals = xvals, yvals = yvals,
model_cv = xgbcv)
progress$close()
} # End of XGBoost
model_trained(TRUE) # Model has been trained
return(return_list)
}) # End of model()
# Logistic Regression Output -------------------------------------------------
# Model Information
output$logModelInfo <- renderText({
if(!is.null(model())){
information <- model()$model_train
return(paste(capture.output(print(information)), collapse = '\n'))
}})
# Model Accuracy (Text)
output$logModelAccuracy <- renderText({
if(!is.null(model())){
accuracy <- model()$confusion_matrix
return(paste(capture.output(print(accuracy)), collapse = '\n'))
}})
# Model Accuracy (Plot)
output$logModelMatrix <- renderPlot({
if(!is.null(model())){
accuracy <- model()$confusion_matrix
accuracy <- as_tibble(accuracy$table)
cvms::plot_confusion_matrix(accuracy, target_col = "Reference",
prediction_col = "Prediction", counts_col = "n")
}}, height = 600, res = 100)
# Variable Importance (Text)
output$logVarImportance <- renderText({
if(!is.null(model())){
varImportance <- varImp(model()$model_train)
return(paste(capture.output(print(varImportance)), collapse = '\n'))
}})
output$logVarImpPlot <- renderPlot({
if(!is.null(model())){
varImportance <- varImp(model()$model_train)
plot(varImportance, top = 10, main = "Variable Importance Plot (Logistic Regression)")
}}, res = 100)
# Matrix Plot
output$logMatrixPlot <- renderPlot({
if(!is.null(model())){
return(plot_confusion_matrix(model()$confusion_matrix)) # Using custom function script
}}, res = 100)
# ROC Plot
output$logRocPlot <- renderPlot({
if(!is.null(model())){
predicted <- model()$model_test
actual <- model()$test_data[, model()$loan_default]
return(plot_roc_curve(predicted, actual)) # Using custom function script
}}, res = 100)
# Training Data
output$logTrainingData <- renderDataTable({
datatable(model()$training_data, options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Test Data
output$logTestData <- renderDataTable({
datatable(model()$test_data, options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Decision Tree
output$logTree <- renderPlot({
if(!is.null(model())){
model <- model()$model_train$finalModel
fancyRpartPlot(model, main = "Decision Tree", sub = NULL,
palettes = c("Greys", "Oranges"))
}})
# Random Forest Output -------------------------------------------------------
# Model Information
output$forestModelInfo <- renderText({
if(!is.null(model())){
information <- model()$model_train
return(paste(capture.output(print(information)), collapse = '\n'))
}})
# Model Accuracy (Text)
output$forestModelAccuracy <- renderText({
if(!is.null(model())){
accuracy <- model()$confusion_matrix
return(paste(capture.output(print(accuracy)), collapse = '\n'))
}})
# Model Accuracy (Plot)
output$forestModelMatrix <- renderPlot({
if(!is.null(model())){
accuracy <- model()$confusion_matrix
accuracy <- as_tibble(accuracy$table)
cvms::plot_confusion_matrix(accuracy, target_col = "Reference",
prediction_col = "Prediction", counts_col = "n")
}}, height = 600, res = 100)
# Variable Importance (Text)
output$forestVarImportance <- renderText({
if(!is.null(model())){
varImportance <- varImp(model()$model_train)
return(paste(capture.output(print(varImportance)), collapse = '\n'))
}})
# Variable Importance (Plot)
output$forestVarImpPlot <- renderPlot({
if(!is.null(model())){
model <- model()$model_train
varImpPlot(model, sort = TRUE, n.var = 10, pch = 16,
main = "Variable Importance Plot (Random Forest)")
}}, res = 100)
# Matrix Plot
output$forestMatrixPlot <- renderPlot({
if(!is.null(model())){
return(plot_confusion_matrix(model()$confusion_matrix)) # Using custom function script
}}, res = 100)
# ROC Plot
output$forestRocPlot <- renderPlot({
if(!is.null(model())){
predicted <- model()$model_test
actual <- model()$test_data[, model()$loan_default]
return(plot_roc_curve(predicted, actual)) # Using custom function script
}}, res = 100)
# Training Data
output$forestTrainingData <- renderDataTable({
datatable(model()$training_data, options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Test Data
output$forestTestData <- renderDataTable({
datatable(model()$test_data, options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Decision Tree
output$forestTree <- renderPlot({
if(!is.null(model())){
decision_trees <- list()
model <- model()$model_train
ntree_num <- model()$ntree
# Getting all the trees made by the model and storing them as rpart objects
# in a list
for(i in 1:ntree_num){
tree <- getTree(model, k = i, labelVar = TRUE)
tree <- rpart(tree)
decision_trees[[i]] <- tree
}
# Plotting tree based on slider input
rpart.plot(decision_trees[[input$treeNumber]], type = 3,
clip.right.labs = FALSE, roundint = FALSE)
}})
# XGboost Output -------------------------------------------------------------
# Model Information
output$boostModelInfo <- renderText({
if(!is.null(model())){
information <- model()$model_train
return(paste(capture.output(print(information)), collapse = '\n'))
}})
# Model Accuracy (Text)
output$boostModelAccuracy <- renderText({
if(!is.null(model())){
accuracy <- model()$confusion_matrix
return(paste(capture.output(print(accuracy)), collapse = '\n'))
}})
# Model Accuracy (Plot)
output$boostModelMatrix <- renderPlot({
if(!is.null(model())){
accuracy <- model()$confusion_matrix
accuracy <- as_tibble(accuracy$table)
cvms::plot_confusion_matrix(accuracy, target_col = "Reference",
prediction_col = "Prediction", counts_col = "n")
}}, height = 600, res = 100)
# Variable Importance (Text)
output$boostVarImp <- renderText({
if(!is.null(model())){
model <- model()$model_train
training <- model()$training_data
importance_matrix <- xgb.importance(model = model)
return(paste(capture.output(print(importance_matrix)), collapse = '\n'))
}})
# Variable Importance (Plot)
output$boostVarImpPlot <- renderPlot({
if(!is.null(model())){
model <- model()$model_train
training <- model()$training_data
importance_matrix <- xgb.importance(model = model)
xgb.plot.importance(importance_matrix, top_n = 10,
main = "Variable Importance Plot (XGBoost)")
}}, res = 100)
# Matrix Plot
output$boostMatrixPlot <- renderPlot({
if(!is.null(model())){
return(plot_confusion_matrix(model()$confusion_matrix)) # Using custom function script
}}, res = 100)
# ROC Plot
output$boostRocPlot <- renderPlot({
if(!is.null(model())){
predicted <- model()$model_test
actual <- model()$test_data[, model()$loan_default]
return(plot_roc_curve(predicted, actual)) # Using custom function script
}}, res = 100)
# Training Data
output$boostTrainingData <- renderDataTable({
datatable(model()$training_data, options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Test Data
output$boostTestData <- renderDataTable({
datatable(model()$test_data, options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Decision Tree
output$boostTree <- renderPlot({
if(!is.null(model())){
decision_tree <- list()
boost_model <- model()$model_train
iter <- model()$model_train$niter
# Getting all the trees made by the model and storing them in a list
for(i in 1:iter){
decision_tree[[i]] <- xgb.plot.tree(model = boost_model, trees = i - 1)
}
# Plotting tree based on slider input
decision_tree[[input$boostTreeNumber]]
}}, res = 100)
# Probability of default -----------------------------------------------------
# Reading in new dataset -----------------------------------------------------
dataset_new <- reactive({
# If a dataset has NOT been uploaded an error message will be returned
# If a dataset is NOT in CSV format an error message will be returned
# If a dataset has been uploaded AND is in CSV format it will be read in
if(is.null(input$probData)){
shinyalert::shinyalert(title = "Oops", text = "Please upload a dataset",
type = "error", showCancelButton = FALSE)
return(NULL)
} else if(!grepl("\\.csv$", input$probData$name, ignore.case = TRUE)){
shinyalert::shinyalert(title = "Oops", text = "Please upload a CSV file",
type = "error", showCancelButton = FALSE)
return(NULL)
} else { # Handling potential errors while reading the datafile
tryCatch({
df_prob <- read.csv(input$probData$datapath)
}, error = function(e){
shinyalert::shinyalert(title = "Oops", text = paste("Error while reading the CSV file: ", e$message),
type = "error", showCancelButton = FALSE)
return(NULL)
}) # End of tryCatch
return(df_prob)
}
}) # End of dataset_new()
# Generating predictions -----------------------------------------------------
predictions <- eventReactive(input$predict, {
# If a dataset has NOT been uploaded an error message will be returned
# If a dataset is NOT in CSV format an error message will be returned
# If a model has been previously trained an error message will be returned
if(is.null(input$probData)){
shinyalert::shinyalert(title = "Oops", text = "Please upload a dataset",
type = "error", showCancelButton = FALSE)
return(NULL)
} else if(!grepl("\\.csv$", input$probData$name, ignore.case = TRUE)){
shinyalert::shinyalert(title = "Oops", text = "Please upload a CSV file",
type = "error", showCancelButton = FALSE)
return(NULL)
} else if(!model_trained()){
shinyalert::shinyalert(title = "Oops", text = "Model has not been trained yet. Please train a model",
type = "error", showCancelButton = FALSE)
return(NULL)
}
df_prob <- dataset_new() # Getting data
model <- model()$model_train
# If logistic regression or random forest was run
if(input$modelSelection != "XGBoost") {
prob_predict <- predict(model, newdata = df_prob, type = "prob")
prob_predict <- round(prob_predict, digits = 2)
} else{
# Changing to matrix format
df_prob_matrix <- xgb.DMatrix(data = as.matrix(df_prob))
# Predicting probability
prob_predict <- predict(model, newdata = df_prob_matrix, type = "prob")
# Converting to dataframe
prob_predict <- data.frame("Default" = 1- prob_predict,
"Non Default" = prob_predict)
prob_predict <- round(prob_predict, digits = 2)
}
df_prob <- cbind(prob_predict, df_prob)
return(df_prob)
})
# Outputting predictions
output$probability <- renderDataTable({
datatable(predictions(), options = list(scrollX = TRUE, paginate = TRUE,
pageLength = 10))
})
# Downloading probability dataset
output$downloadprobDataset <- downloadHandler(
filename = function(){
paste("default_probability_dataset.csv", sep = "")
},
content = function(file){
prob_data <- predictions()
write.csv(prob_data, file, row.names = FALSE)
})
} # End of Server()
shinyApp(ui = ui, server = server)
runApp('03__Shiny/01__app.R')
