training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- c(100, 200, 300, 400, 500, 600, 700) # Number of trees
mtry_values <- c(3, 5, 7, 8, 9, 10) # Number of variables to consider at each split
best_accuracy <- 0
best_model <- NULL
best_ntree <- NULL
best_mtry <- NULL
# The below code will utilize the feature selected variables and also cycle
# through the above values and select the best performing model
for (ntree in ntree_values) {
for (mtry in mtry_values) {
model <- randomForest(formula = getNonRejectedFormula(f_selection),
data = training_set, ntree = ntree, mtry = mtry)
predicted <- predict(model, newdata = test_set)
confusion_matrix <- table(predicted, test_set$Class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model
best_ntree < ntree_values
best_mtry <-  mtry_values
}
}
}
predictions <- predict(best_model, newdata = test_set)
confusionMatrix(table(predictions, test_set$Class))
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Feature Selection ------------------------------------------------------------
set.seed(123)
# Performing feature selection to select most important variables
f_selection <- Boruta(Class ~ ., data = german_data,
doTrace = 2, maxRuns = 500)
# Splitting data ---------------------------------------------------------------
set.seed(147)
# Store row numbers for training set: index_train
index_train <- sample(1:nrow(german_data), 2 / 3 * nrow(german_data))
# Create training set: training_set
training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- c(100, 200, 300, 400, 500, 600, 700) # Number of trees
mtry_values <- c(3, 5, 7, 8, 9, 10) # Number of variables to consider at each split
best_accuracy <- 0
best_model <- NULL
best_ntree <- NULL
best_mtry <- NULL
# The below code will utilize the feature selected variables and also cycle
# through the above values and select the best performing model
for (ntree in ntree_values) {
for (mtry in mtry_values) {
model <- randomForest(formula = getNonRejectedFormula(f_selection),
data = training_set, ntree = ntree, mtry = mtry)
predicted <- predict(model, newdata = test_set)
confusion_matrix <- table(predicted, test_set$Class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model
best_ntree < ntree
best_mtry <-  mtry
}
}
}
predictions <- predict(best_model, newdata = test_set)
confusionMatrix(table(predictions, test_set$Class))
predictions
best_model
best_ntree
confusionMatrix(table(predictions, test_set$Class))
?confusionMatrix
print(f_selection)
?Boruta
ls("package:Boruta")
getSelectedAttributes(f_selection)
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Feature Selection ------------------------------------------------------------
set.seed(123)
# Performing feature selection to select most important variables
f_selection <- Boruta(Class ~ ., data = german_data,
doTrace = 2, maxRuns = 500)
getSelectedAttributes(f_selection)
test <- Boruta(Class ~ ., data = german_data, doTrace = 2)
test <- Boruta(Class ~ ., data = german_data, doTrace = 2, maxRuns = 1000)
getSelectedAttributes(test)
getSelectedAttributes(f_selection)
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Feature Selection ------------------------------------------------------------
set.seed(123)
# Performing feature selection to select most important variables
f_selection <- Boruta(Class ~ ., data = german_data,
doTrace = 2, maxRuns = 500)
# Viewing output and the selected variables
# print(f_selection)
# getSelectedAttributes(f_selection)
f_selection_final <- TentativeRoughFix(f_selection)
getSelectedAttributes(f_selection_final)
getSelectedAttributes(f_selection)
?TentativeRoughFix
?getNonRejectedFormula(f_selection_final)
getNonRejectedFormula(f_selection_final)
# Splitting data ---------------------------------------------------------------
set.seed(147)
# Store row numbers for training set: index_train
index_train <- sample(1:nrow(german_data), 2 / 3 * nrow(german_data))
# Create training set: training_set
training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- c(100, 200, 300, 400, 500, 600, 700) # Number of trees
mtry_values <- c(3, 5, 7, 8, 9, 10) # Number of variables to consider at each split
best_accuracy <- 0
best_model <- NULL
# The below code will utilize the feature selected variables and also cycle
# through the above values and select the best performing model
for (ntree in ntree_values) {
for (mtry in mtry_values) {
model <- randomForest(formula = getNonRejectedFormula(f_selection_final),
data = training_set, ntree = ntree, mtry = mtry)
predicted <- predict(model, newdata = test_set)
confusion_matrix <- table(predicted, test_set$Class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model
}
}
}
best_model
predictions <- predict(best_model, newdata = test_set)
confusionMatrix(table(predictions, test_set$Class))
?arrange
# Testing model
new_data <- german_data %>%
arrange(desc(Class))
new_data
split_25 <- round(num_rows * 0.25)
# Getting the first 25% and the last 25%
num_rows <- nrow(new_data)
split_25 <- round(num_rows * 0.25)
new_data_25 <- new_data %>%
slice(1:split_25)
View(new_data_25)
table(new_data_25$Class)
new_data_75 <- new_data %>%
slice((num_rows - split_25 + 1):num_rows)
table(new_data_75)
table(new_data_75$Class)
new_data_combined <- bind_rows(new_data_25, new_data_75)
table(new_data_combined$Class)
new_predictions <- predict(best_model, newdata = new_data_combined)
confusionMatrix(table(new_predictions, new_data_combined$Class))
# Define the parameter grid
tuning_grid <- expand.grid(
ntree = ntree_values,
mtry = mtry_values
)
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Feature Selection ------------------------------------------------------------
set.seed(123)
# Performing feature selection to select most important variables
f_selection <- Boruta(Class ~ ., data = german_data, doTrace = 2, maxRuns = 500)
# Assessing what to do with the "Tentative" labelled variables
f_selection_final <- TentativeRoughFix(f_selection)
# Viewing output and the selected variables
# print(f_selection_final)
# getSelectedAttributes(f_selection_final)
# Splitting data ---------------------------------------------------------------
set.seed(147)
# Store row numbers for training set: index_train
index_train <- sample(1:nrow(german_data), 2 / 3 * nrow(german_data))
# Create training set: training_set
training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- seq(100, 1000, by = 100) # Number of trees
mtry_values <- seq(1, 10, by = 1) # Number of variables to consider at each split
# Creating all possible combinations of the above values
combinations <- expand.grid(ntree = ntree_values, mtry = mtry_values)
# Define the parameter grid
tuning_grid <- expand.grid(
ntree = ntree_values,
mtry = mtry_values
)
# Define train control for cross-validation
ctrl <- trainControl(
method = "cv", # Cross-validation method
number = 5, # Number of folds
verboseIter = TRUE # Print progress
)
# Perform model tuning
tuned_model <- train(
formula = getNonRejectedFormula(f_selection_final),
data = training_set,
method = "rf", # Random forest method
trControl = ctrl,
tuneGrid = tuning_grid,
metric = "Accuracy" # Evaluation metric
)
tuning_grid
?train
getModelInfo()
modelLookup("rf")
# Perform model tuning
tuned_model <- train(
formula = getNonRejectedFormula(f_selection_final),
data = training_set,
method = "rf", # Random forest method
trControl = ctrl,
tuneGrid = tuning_grid,
metric = "Accuracy" # Evaluation metric
)
tuning_grid
ctrl
# Define the parameter grid
tuning_grid <- expand.grid(
mtry = mtry_values
)
# Define train control for cross-validation
ctrl <- trainControl(
method = "cv", # Cross-validation method
number = 5, # Number of folds
verboseIter = TRUE # Print progress
)
# Perform model tuning
tuned_model <- train(
formula = getNonRejectedFormula(f_selection_final),
data = training_set,
method = "rf", # Random forest method
trControl = ctrl,
tuneGrid = tuning_grid,
metric = "Accuracy" # Evaluation metric
)
# Retrieve the best model
best_model <- tuned_model$bestTune
# Perform model tuning
tuned_model <- train(
form = getNonRejectedFormula(f_selection_final),
data = training_set,
method = "rf", # Random forest method
trControl = ctrl,
tuneGrid = tuning_grid,
metric = "Accuracy" # Evaluation metric
)
# Retrieve the best model
best_model <- tuned_model$bestTune
# Print the best model parameters
print(best_model)
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Feature Selection ------------------------------------------------------------
set.seed(123)
# Performing feature selection to select most important variables
f_selection <- Boruta(Class ~ ., data = german_data, doTrace = 2, maxRuns = 500)
# Assessing what to do with the "Tentative" labelled variables
f_selection_final <- TentativeRoughFix(f_selection)
# Viewing output and the selected variables
# print(f_selection_final)
# getSelectedAttributes(f_selection_final)
# Splitting data ---------------------------------------------------------------
set.seed(147)
# Store row numbers for training set: index_train
index_train <- sample(1:nrow(german_data), 2 / 3 * nrow(german_data))
# Create training set: training_set
training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- seq(100, 1000, by = 100) # Number of trees
mtry_values <- seq(1, 10, by = 1) # Number of variables to consider at each split
# Creating all possible combinations of the above values
combinations <- expand.grid(ntree = ntree_values, mtry = mtry_values)
# For storing output
best_accuracy <- 0
best_model <- NULL
# The below loop with iterate through all possible combinations of the tree/split
# values and find the best performing model
for (i in 1:nrow(combinations)) {
ntree <- combinations$ntree[i]
mtry <- combinations$mtry[i]
model <- randomForest(formula = getNonRejectedFormula(f_selection_final),
data = training_set, ntree = ntree, mtry = mtry)
predicted <- predict(model, newdata = test_set)
confusion_matrix <- table(predicted, test_set$Class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# If a model has better accuracy it is saved
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model
}
}
# Performing predictions and evaluating ----------------------------------------
predictions <- predict(best_model, newdata = test_set)
confusion_matrix <- confusionMatrix(table(predictions, test_set$Class))
# Outputting -------------------------------------------------------------------
best_model
confusion_matrix
?readline
readline()
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
perform_selection <- readline(prompt = "Do you want to perform feature selection: (1 = yes; 2 = no")
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Splitting data ---------------------------------------------------------------
set.seed(147)
# Store row numbers for training set: index_train
index_train <- sample(1:nrow(german_data), 2 / 3 * nrow(german_data))
# Create training set: training_set
training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- seq(100, 1000, by = 100) # Number of trees
mtry_values <- seq(1, 10, by = 1) # Number of variables to consider at each split
# Creating all possible combinations of the above values
combinations <- expand.grid(ntree = ntree_values, mtry = mtry_values)
# For storing output
best_accuracy <- 0
best_model <- NULL
# The below loop with iterate through all possible combinations of the tree/split
# values and find the best performing model
for (i in 1:nrow(combinations)) {
ntree <- combinations$ntree[i]
mtry <- combinations$mtry[i]
model <- randomForest(formula = Class ~ .,
data = training_set, ntree = ntree, mtry = mtry)
predicted <- predict(model, newdata = test_set)
confusion_matrix <- table(predicted, test_set$Class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# If a model has better accuracy it is saved
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model
}
}
# Performing predictions and evaluating ----------------------------------------
predictions <- predict(best_model, newdata = test_set)
confusion_matrix <- confusionMatrix(table(predictions, test_set$Class))
# Outputting -------------------------------------------------------------------
best_model
confusion_matrix
# Creating a random forest decision tree with feature selection
rm(list = ls())
library(dplyr)
library(caTools)
library(randomForest)
library(caret)
library(Boruta) # for feature selection
# Reading in data --------------------------------------------------------------
path_data <- "./01__Data/01__German_Credit_Data/"
german_data <- read.csv(file.path(path_data, "01__GCD-Binary.csv"))
# Processing -------------------------------------------------------------------
german_data <- german_data %>%
mutate(Class = recode(Class, "Bad" = 0, "Good" = 1)) %>%
mutate(Class = as.factor(Class))
# Feature Selection ------------------------------------------------------------
set.seed(123)
# Performing feature selection to select most important variables
f_selection <- Boruta(Class ~ ., data = german_data, doTrace = 2, maxRuns = 500)
# Assessing what to do with the "Tentative" labelled variables
f_selection_final <- TentativeRoughFix(f_selection)
# Viewing output and the selected variables
# print(f_selection_final)
# getSelectedAttributes(f_selection_final)
# Splitting data ---------------------------------------------------------------
set.seed(147)
# Store row numbers for training set: index_train
index_train <- sample(1:nrow(german_data), 2 / 3 * nrow(german_data))
# Create training set: training_set
training_set <- german_data[index_train, ]
# Create test set: test_set
test_set <- german_data[-index_train, ]
# Performing random forest -----------------------------------------------------
set.seed(148)
# Assessing the accuracy of different tree sizes and splits
ntree_values <- seq(100, 1000, by = 100) # Number of trees
mtry_values <- seq(1, 10, by = 1) # Number of variables to consider at each split
# Creating all possible combinations of the above values
combinations <- expand.grid(ntree = ntree_values, mtry = mtry_values)
# For storing output
best_accuracy <- 0
best_model <- NULL
# The below loop with iterate through all possible combinations of the tree/split
# values and find the best performing model
for (i in 1:nrow(combinations)) {
ntree <- combinations$ntree[i]
mtry <- combinations$mtry[i]
model <- randomForest(formula = getNonRejectedFormula(f_selection_final),
data = training_set, ntree = ntree, mtry = mtry)
predicted <- predict(model, newdata = test_set)
confusion_matrix <- table(predicted, test_set$Class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# If a model has better accuracy it is saved
if (accuracy > best_accuracy) {
best_accuracy <- accuracy
best_model <- model
}
}
# Performing predictions and evaluating ----------------------------------------
predictions <- predict(best_model, newdata = test_set)
confusion_matrix <- confusionMatrix(table(predictions, test_set$Class))
# Outputting -------------------------------------------------------------------
best_model
confusion_matrix
table(training_set$Class)
table(german_data$Class)
